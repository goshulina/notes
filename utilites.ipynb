{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import torch\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.5 (default, Sep  4 2020, 02:22:02) \\n[Clang 10.0.0 ]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AnyData(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # xy = np.loadtxt('', delimiter='', dtype=np.float32, skiprows=1)\n",
    "        # self.x\n",
    "        # self.y\n",
    "        # self.n_namples = xy.shape[0]\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # return self.x[index], self.y[index]\n",
    "        pass\n",
    "        \n",
    "    def __len__(self):\n",
    "        # return self.n_namples\n",
    "        pass\n",
    "    \n",
    "dataset = AnyData()\n",
    "# dataset[0]\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "# dataiter = iter(dataloader)\n",
    "# data_1 = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.eye(10)\n",
    "x.sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# counts = Counter(words)\n",
    "# cv = CountVectorizer(vocabulary=counts.keys())\n",
    "# cv.fit_transform(i).toarray()[0].tolist()\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5,\n",
    "    min_df=0.0005,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "x = vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split & shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(corpus, labels, ratios=False):\n",
    "    X_train, X_test, y_train, y_test = [], [], [], []\n",
    "    if ratios:\n",
    "        for k, v in ratios.items():\n",
    "            train_rate = v[1] / v[0]\n",
    "            corpus_current, labels_current = data_by_class_selection(corpus, labels, k)\n",
    "            x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(\n",
    "                corpus_current, \n",
    "                labels_current, \n",
    "                test_size=train_rate, \n",
    "                random_state=41)\n",
    "            X_train.extend(x_train_1)\n",
    "            X_test.extend(x_test_1)\n",
    "            y_train.extend(y_train_1)\n",
    "            y_test.extend(y_test_1)\n",
    "    else:\n",
    "        for i in dict(pd.Series(labels).value_counts()).keys():\n",
    "            corpus_current, labels_current = data_by_class_selection(corpus, labels, i)\n",
    "            train_rate = 0.3\n",
    "            x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(\n",
    "                corpus_current, \n",
    "                labels_current, \n",
    "                test_size=train_rate, \n",
    "                random_state=41)\n",
    "            X_train.extend(x_train_1)\n",
    "            X_test.extend(x_test_1)\n",
    "            y_train.extend(y_train_1)\n",
    "            y_test.extend(y_test_1)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def data_by_class_selection(corpus, labels, c):\n",
    "    new_corpus = []\n",
    "    new_labels = []\n",
    "    for d, l in zip(corpus, labels):\n",
    "        if l == c:\n",
    "            new_corpus.append(d)\n",
    "            new_labels.append(l)\n",
    "    return new_corpus, new_labels\n",
    "\n",
    "def shuffle_data(corpus, labels):\n",
    "    combined = list(zip(corpus, labels))\n",
    "    random.shuffle(combined)\n",
    "    corpus[:], labels[:] = zip(*combined)\n",
    "    return corpus, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import re\n",
    "stemmer = Mystem()\n",
    "def preprocess(t, stem=True):\n",
    "    x = ''.join([i for i in t.lower() if i in 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя .'])  # <<<<<<< LOWER\n",
    "    x = x.replace('.', ' ')\n",
    "    if stem:\n",
    "        x = ''.join(stemmer.lemmatize(x))\n",
    "    else:\n",
    "        pass\n",
    "    x = ' '.join([i for i in x.split() if len(i) > 2])\n",
    "    x = re.sub('\\s+', ' ', x)\n",
    "    return x.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tesseract(x):\n",
    "    import re\n",
    "    x = re.sub(r'\\\"', ' ', x)\n",
    "    x = x.replace('\\\\n', ' ')\n",
    "    x = x.replace('\\\\', ' ')\n",
    "    x = re.sub('\\s+', ' ', x)\n",
    "    return x.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines (classic ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "folds = 2\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(analyzer = 'word', min_df=0)),#, max_df=0.9, max_features=10000)),\n",
    "    # ('LinearSVC', LinearSVC(C=1, random_state=42))\n",
    "    ('LogisticRegression', LogisticRegression(solver='liblinear', random_state=42))\n",
    "    # ('SGDClassifier', SGDClassifier(loss='hinge', alpha = 0.001))# np.linspace(0.01, 0.05, 10)\n",
    "])\n",
    "parameters = { \n",
    "    # 'tfidf__max_df': (0.9, 1), # 0.8 <- 0.95 ### 0.6 \n",
    "    # 'tfidf__min_df': (0), # 0.01 <- 0.05 ### 0.005 <- 0.01 ### 0.001\n",
    "    'tfidf__ngram_range': [(1, 2), (1, 3)]\n",
    "    # 'svm__decision_function_shape': ['ovo', 'ovr'],\n",
    "    # 'svm__C': (4, 6)\n",
    "    # 'tfidf__max_features': (10000, 32000)\n",
    "}\n",
    "c_val = StratifiedKFold(n_splits=folds)\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=c_val, verbose=3, n_jobs=4, scoring='f1_macro')\n",
    "grid_search_tune.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "print(grid_search_tune.best_estimator_.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "class TextClassifier(BaseEstimator):\n",
    "    @staticmethod\n",
    "    def _build_model():\n",
    "        model = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(analyzer='word', min_df=0, max_df=0.6, ngram_range=(1, 3))),\n",
    "            ('RandomUnderSampler', RandomUnderSampler()),\n",
    "            ('Smote', SMOTE()),\n",
    "            ('svm', SVC(C=3, kernel='linear', decision_function_shape='ovr'))\n",
    "        ])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = TextClassifier._build_model()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.save(path)\n",
    "\n",
    "    def load(self, path):\n",
    "        return self.model.load_model(path)\n",
    "model = TextClassifier()._build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(analyzer = 'word', min_df=0, max_df=0.6, ngram_range=(1, 3))),\n",
    "    ('RandomUnderSampler', RandomUnderSampler()),\n",
    "    ('Smote', SMOTE()),\n",
    "    ('svm', SVC(C=3, kernel='linear', decision_function_shape='ovr'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "m = confusion_matrix(y_true, y_pred)\n",
    "new_m = []\n",
    "l = []\n",
    "print(m)\n",
    "for i in m:\n",
    "    s = sum(i)\n",
    "    for ii in i:\n",
    "        l.append(round(ii / s * 100))\n",
    "    new_m.append(l)\n",
    "    l = []\n",
    "print(np.array(new_m))\n",
    "\n",
    "print(f1_score(y_true, y_pred, average='macro'))\n",
    "print(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    \n",
    "cnf_matrix = np.array(new_m).astype(int)\n",
    "\n",
    "    \n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(num=None, figsize=(13, 11), dpi=80, facecolor='w', edgecolor='k')\n",
    "plot_confusion_matrix(cnf_matrix, classes=list(pd.Series(y_pred).value_counts().keys()),\n",
    "                      title='Confusion matrix huyatrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jaccard(x1, x2):\n",
    "    intersection = set(x1.split()).intersection(set(x2.split()))\n",
    "    union = set(x1.split()).union(set(x2.split()))\n",
    "    try:\n",
    "        return float(format(len(intersection) / len(union), '.2f'))\n",
    "    except ZeroDivisionError:\n",
    "        return 233.\n",
    "jaccard('Дяденька король цветной капусты хотца', 'Дяденька король цветной капусты')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(path, ext):\n",
    "    import os\n",
    "    paths_to_files = []\n",
    "    for root, dir, files in os.walk(path):\n",
    "        if files:\n",
    "            for f in files:\n",
    "                if ext in f:\n",
    "                    paths_to_files.append(os.path.join(root, f))\n",
    "    if paths_to_files:\n",
    "        return paths_to_files\n",
    "    else:\n",
    "        raise ValueError('No such directory')\n",
    "# get_files('<path>', 'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('personal_info_wall_media_sep152020.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    data.update(user_data)\n",
    "    json.dump(data, open(\"personal_info_wall_media_sep152020.json\", 'w'))\n",
    "except FileNotFoundError:\n",
    "    json.dump(user_data, open(\"personal_info_wall_media_sep152020.json\", 'w'))\n",
    "user_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "new_labels1, new_corpus1 = [], []\n",
    "with open('type_march_.csv', 'rt') as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    for n, i in enumerate(readCSV):\n",
    "        # print(i)\n",
    "        # break\n",
    "        if n != 0:\n",
    "            new_labels1.append(i[0])\n",
    "            new_corpus1.append(i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in t for item in sublist]\n",
    "{k: v for k, v in sorted(result.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "def top_n_max(series, top=2, diraction=max):\n",
    "    series = list(set(series.tolist()))\n",
    "    if diraction == max:\n",
    "      top_result = sorted(series)[-top:]\n",
    "    else:\n",
    "      top_result = sorted(series)[:top]\n",
    "    return top_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
